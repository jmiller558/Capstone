{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2hIZAcDl2t-"
      },
      "source": [
        "**IMPORT PACKAGES | BUILD CUSTOM FUNCTIONS | SET PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7jZlcUq-yWt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import datetime\n",
        "import lightgbm as lgb\n",
        "import random\n",
        "import os\n",
        "import psutil\n",
        "import argparse\n",
        "import time\n",
        "import warnings\n",
        "import gc\n",
        "import pickle\n",
        "import math\n",
        "import shutil\n",
        "import math, decimal\n",
        "\n",
        "from math import ceil\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8q54FH0f4v1"
      },
      "outputs": [],
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "os.environ['PYTHONHASHSEED'] = str(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmaLAOnTMUnE"
      },
      "outputs": [],
      "source": [
        "class Util(object):\n",
        "    @staticmethod\n",
        "    def set_seed(seed):\n",
        "        random.seed(seed)\n",
        "        np.random.seed(seed)\n",
        "        os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def get_memory_usage():\n",
        "        return np.round(psutil.Process(os.getpid()).memory_info()[0] / 2. ** 30, 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def reduce_mem_usage(df, verbose=False):\n",
        "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "        start_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                        df[col] = df[col].astype(np.int8)\n",
        "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "        end_mem = df.memory_usage().sum() / 1024 ** 2\n",
        "        if verbose:\n",
        "            print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (\n",
        "                    start_mem - end_mem) / start_mem))\n",
        "        return df\n",
        "\n",
        "    @staticmethod\n",
        "    def merge_by_concat(df1, df2, merge_on):\n",
        "        merged_gf = df1[merge_on]\n",
        "        merged_gf = merged_gf.merge(df2, on=merge_on, how='left')\n",
        "        new_columns = [col for col in list(merged_gf) if col not in merge_on]\n",
        "        df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)\n",
        "        return df1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pjGiFFZi7Zq"
      },
      "source": [
        "**OPTION TO DOWNLOAD DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGcLX8D1BrPM",
        "outputId": "d3e7f548-c743-4edb-8c91-2d37176e1dd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dask_xgboost\n",
            "  Downloading dask_xgboost-0.2.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.8/dist-packages (from dask_xgboost) (2022.2.0)\n",
            "Requirement already satisfied: distributed>=1.15.2 in /usr/local/lib/python3.8/dist-packages (from dask_xgboost) (2022.2.0)\n",
            "Requirement already satisfied: xgboost<=0.90 in /usr/local/lib/python3.8/dist-packages (from dask_xgboost) (0.90)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (1.0.4)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (1.5.0)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (6.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (21.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (2.4.0)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (0.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (5.4.8)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (2.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (2.11.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from distributed>=1.15.2->dask_xgboost) (57.4.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.8/dist-packages (from dask->dask_xgboost) (1.3.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from dask->dask_xgboost) (2022.11.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->distributed>=1.15.2->dask_xgboost) (3.0.9)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.8/dist-packages (from partd>=0.3.10->dask->dask_xgboost) (1.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from xgboost<=0.90->dask_xgboost) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from xgboost<=0.90->dask_xgboost) (1.21.6)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.8/dist-packages (from zict>=0.1.3->distributed>=1.15.2->dask_xgboost) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->distributed>=1.15.2->dask_xgboost) (2.0.1)\n",
            "Installing collected packages: dask-xgboost\n",
            "Successfully installed dask-xgboost-0.2.0\n"
          ]
        }
      ],
      "source": [
        "# %%bash\n",
        "!pip install dask_xgboost\n",
        "# pip install kaggle\n",
        "# export KAGGLE_USERNAME=jmiller558\n",
        "# export KAGGLE_KEY=812fcd89e3a0fc00cb629bf2306b215e\n",
        "\n",
        "# kaggle competitions download -c m5-forecasting-accuracy\n",
        "\n",
        "# unzip -n m5-forecasting-accuracy -d m5-forecasting-accuracy\n",
        "# rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyKatBJ6i_9e"
      },
      "source": [
        "**OPTION TO MOUNT DRIVE WITH DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFq6kjNqhudj",
        "outputId": "d3a19e3a-0901-4604-90f9-8e08c1385465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGNv8eNojn89"
      },
      "source": [
        "**PATHS FOR DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKebCVdEjnc-"
      },
      "outputs": [],
      "source": [
        "#input paths for base data\n",
        "trainpath = '/content/drive/MyDrive/Capstone/Models/m5-forecasting-accuracy/sales_train_evaluation.csv'\n",
        "trainpath = '/content/drive/MyDrive/Capstone/Models/m5-forecasting-accuracy/sales_train_evaluation.csv'\n",
        "pricepath = '/content/drive/MyDrive/Capstone/Models/m5-forecasting-accuracy/sell_prices.csv'\n",
        "calpath = '/content/drive/MyDrive/Capstone/Models/m5-forecasting-accuracy/calendar.csv'\n",
        "submissionpath = '/content/drive/MyDrive/Capstone/Models/m5-forecasting-accuracy/sample_submission.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DveFC0PCjrKV"
      },
      "outputs": [],
      "source": [
        "#input paths for Feature Engineered DFs\n",
        "grid_base_path = '/content/drive/MyDrive/Capstone/Models/Weekly_lightgbm/grid_base_weekly'\n",
        "calfeats_path = '/content/drive/MyDrive/Capstone/Models/Weekly_lightgbm/calfeats_weekly'\n",
        "pricefeats_path = '/content/drive/MyDrive/Capstone/Models/Weekly_lightgbm/pricefeats_weekly'\n",
        "encoding_path = '/content/drive/MyDrive/Capstone/Models/Weekly_lightgbm/encodingfeats_weekly'\n",
        "lagfeats_path = '/content/drive/MyDrive/Capstone/Models/Weekly_lightgbm/weekly_lagfeats_'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_97Wz_4WNawJ"
      },
      "source": [
        "**LOAD BASE COMPETITION DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXPfpO4-9gjY"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    # self.log.info('load_data')\n",
        "    train_df = pd.read_csv(trainpath)\n",
        "    # self.log.info('train_df.shape', train_df.shape)\n",
        "    prices_df = pd.read_csv(pricepath)\n",
        "    # self.log.info('prices_df.shape', prices_df.shape)\n",
        "    calendar_df = pd.read_csv(calpath)\n",
        "    # self.log.info('calendar_df.shape', calendar_df.shape)\n",
        "    submission_df = pd.read_csv(submissionpath)\n",
        "    # self.log.info('submission_df.shape', submission_df.shape)\n",
        "\n",
        "    return train_df, prices_df, calendar_df, submission_df\n",
        "\n",
        "train_df, prices_df, calendar_df, submission_df = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Variables**"
      ],
      "metadata": {
        "id": "hVfve1n_joHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'demand'\n",
        "prediction_horizon = 12\n",
        "start_train_week_x = 1\n",
        "end_train_week_x = 264\n",
        "\n",
        "remove_features = ['id', 'state_id', 'store_id', 'wm_yr_wk','week', target]"
      ],
      "metadata": {
        "id": "y50uiLzPjmXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzQlqfnalod_"
      },
      "source": [
        "**BASE FEATURE ENGINEERING (ONLY NEEDED FIRST TIME)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsA5z4k9NgOS"
      },
      "source": [
        "\n",
        "\n",
        "*   BUILD GRID_BASE\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_0TwxFAEbyD"
      },
      "outputs": [],
      "source": [
        "index_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
        "grid_df = pd.melt(train_df, id_vars=index_columns, var_name='d', value_name=target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4bPNTXtLkVF"
      },
      "outputs": [],
      "source": [
        "release_df = prices_df.groupby(['store_id', 'item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
        "release_df.columns = ['store_id', 'item_id', 'release']\n",
        "grid_df = Util.merge_by_concat(grid_df, release_df, ['store_id', 'item_id'])\n",
        "del release_df\n",
        "grid_df = Util.merge_by_concat(grid_df, calendar_df[['wm_yr_wk', 'd']], ['d'])\n",
        "grid_df = grid_df.reset_index(drop=True)\n",
        "grid_df['release'] = grid_df['release'] - grid_df['release'].min()\n",
        "grid_df['release'] = grid_df['release'].astype(np.int16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG7CJ6ojLZuC"
      },
      "outputs": [],
      "source": [
        "grid_df = grid_df[grid_df.wm_yr_wk<=11616]\n",
        "weekly_df = grid_df[['id','wm_yr_wk','demand']]\n",
        "weekly_df = weekly_df.groupby(['id','wm_yr_wk']).sum().reset_index()\n",
        "grid_to_merge = grid_df[index_columns+['release']].drop_duplicates()\n",
        "weekly_df = weekly_df.merge(grid_to_merge, on=['id'], how='left')[index_columns+['release','wm_yr_wk','demand']]\n",
        "week_df = pd.DataFrame(data={'wm_yr_wk':weekly_df.wm_yr_wk.unique(),'week':range(len(weekly_df.wm_yr_wk.unique()))})\n",
        "weekly_df = weekly_df.merge(week_df, on=['wm_yr_wk'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WZ1nHWMMepX"
      },
      "outputs": [],
      "source": [
        "for col in index_columns:\n",
        "        weekly_df[col] = weekly_df[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL7Mrm0Gj85M"
      },
      "outputs": [],
      "source": [
        "weekly_df.to_pickle(grid_base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxU_dhFWhH0v"
      },
      "outputs": [],
      "source": [
        "del grid_df,weekly_df,grid_to_merge,week_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM_8RJ_aYSZO"
      },
      "source": [
        "*   BUILD CAL FEATURES\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cal_base = calendar_df[['date','wm_yr_wk']].groupby(['wm_yr_wk']).agg(['min']).reset_index()\n",
        "cal_base.columns = ['wm_yr_wk', 'wk_start_date']"
      ],
      "metadata": {
        "id": "HbF5zjKAtS0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_full = pd.concat([calendar_df,pd.get_dummies(calendar_df[['event_name_1','event_type_1','event_name_2','event_type_2']])],axis=1)"
      ],
      "metadata": {
        "id": "Rtf_Bf1EruMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_full = cal_full.drop(['date','weekday', 'wday', 'month', 'year', 'd','event_name_1','event_type_1','event_name_2','event_type_2',],axis=1)"
      ],
      "metadata": {
        "id": "w4O1_pmct_ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_final = cal_base.merge(cal_full.groupby('wm_yr_wk').sum().reset_index(), on=['wm_yr_wk'], how='left')"
      ],
      "metadata": {
        "id": "93JVUJO9uect"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyAIhDXjYX56"
      },
      "outputs": [],
      "source": [
        "dec = decimal.Decimal\n",
        "\n",
        "def get_moon_phase(d):  # 0=new, 4=full; 4 days/phase\n",
        "        diff = datetime.datetime.strptime(d, '%Y-%m-%d') - datetime.datetime(2001, 1, 1)\n",
        "        days = dec(diff.days) + (dec(diff.seconds) / dec(86400))\n",
        "        lunations = dec(\"0.20439731\") + (days * dec(\"0.03386319269\"))\n",
        "        phase_index = math.floor((lunations % dec(1) * dec(8)) + dec('0.5'))\n",
        "        return int(phase_index) & 7\n",
        "        \n",
        "cal_final['moon'] = cal_final.wk_start_date.apply(get_moon_phase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef5AajEDZGU1"
      },
      "outputs": [],
      "source": [
        "icols = ['snap_CA',\n",
        "          'snap_TX',\n",
        "          'snap_WI']\n",
        "for col in icols:\n",
        "  cal_final[col] = cal_final[col].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccsgmLdCZNNj"
      },
      "outputs": [],
      "source": [
        "cal_final['wk_start_date'] = pd.to_datetime(cal_final['wk_start_date'])\n",
        "\n",
        "cal_final['tm_d'] = cal_final['wk_start_date'].dt.day.astype(np.int8)\n",
        "cal_final['tm_w'] = cal_final['wk_start_date'].dt.week.astype(np.int8)\n",
        "cal_final['tm_m'] = cal_final['wk_start_date'].dt.month.astype(np.int8)\n",
        "cal_final['tm_y'] = cal_final['wk_start_date'].dt.year\n",
        "cal_final['tm_y'] = (cal_final['tm_y'] - cal_final['tm_y'].min()).astype(np.int8)\n",
        "cal_final['tm_wm'] = cal_final['tm_d'].apply(lambda x: ceil(x / 7)).astype(np.int8)\n",
        "\n",
        "del cal_final['wk_start_date'],cal_final['tm_d']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_df = pd.read_pickle(grid_base_path)[['id','wm_yr_wk']]\n",
        "calfeats_final = grid_df.merge(cal_final, on=['wm_yr_wk'], how='left')"
      ],
      "metadata": {
        "id": "DUiq6RY9IDg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWfN6_u_ZNxg"
      },
      "outputs": [],
      "source": [
        "calfeats_final.to_pickle(calfeats_path)\n",
        "del cal_final,cal_full,cal_base,grid_df,calfeats_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIff0KflNNKr"
      },
      "source": [
        "*   BUILD PRICING FEATURES\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ_1SPJFNk4C"
      },
      "outputs": [],
      "source": [
        "calendar_prices = calendar_df[['wm_yr_wk', 'month', 'year']]\n",
        "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
        "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk', 'month', 'year']], on=['wm_yr_wk'], how='left')\n",
        "del calendar_prices\n",
        "\n",
        "grid_df = pd.read_pickle(grid_base_path)\n",
        "\n",
        "prices_df = prices_df[prices_df['wm_yr_wk']<=grid_df['wm_yr_wk'].max()]\n",
        "\n",
        "prices_df['price_max'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('max')\n",
        "prices_df['price_min'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('min')\n",
        "prices_df['price_std'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('std')\n",
        "prices_df['price_mean'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('mean')\n",
        "prices_df['price_norm'] = prices_df['sell_price'] / prices_df['price_max']\n",
        "prices_df['price_nunique'] = prices_df.groupby(['store_id', 'item_id'])['sell_price'].transform('nunique')\n",
        "prices_df['item_nunique'] = prices_df.groupby(['store_id', 'sell_price'])['item_id'].transform('nunique')\n",
        "\n",
        "prices_df['price_momentum'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id'])[\n",
        "    'sell_price'].transform(lambda x: x.shift(1))\n",
        "prices_df['price_momentum_m'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'month'])[\n",
        "    'sell_price'].transform('mean')\n",
        "prices_df['price_momentum_y'] = prices_df['sell_price'] / prices_df.groupby(['store_id', 'item_id', 'year'])[\n",
        "    'sell_price'].transform('mean')\n",
        "\n",
        "prices_df['sell_price_cent'] = [math.modf(p)[0] for p in prices_df['sell_price']]\n",
        "prices_df['price_max_cent'] = [math.modf(p)[0] for p in prices_df['price_max']]\n",
        "prices_df['price_min_cent'] = [math.modf(p)[0] for p in prices_df['price_min']]\n",
        "\n",
        "del prices_df['month'], prices_df['year']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w54wbcXzN1CY"
      },
      "outputs": [],
      "source": [
        "grid_df = pd.read_pickle(grid_base_path)\n",
        "original_columns = list(grid_df)\n",
        "pricefeats_df = grid_df.merge(prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
        "keep_columns = [col for col in list(pricefeats_df) if col not in original_columns]\n",
        "pricefeats_df = pricefeats_df[['id', 'wm_yr_wk'] + keep_columns]\n",
        "pricefeats_df = Util.reduce_mem_usage(pricefeats_df)\n",
        "del prices_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt5EAKaLnDtM"
      },
      "outputs": [],
      "source": [
        "pricefeats_df.to_pickle(pricefeats_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE8DHKaGhMZx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "outputId": "13c2d29d-df47-466b-95b2-79b48d56eb24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-ce8689f2fc90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpricefeats_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mgrid_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pricefeats_df' is not defined"
          ]
        }
      ],
      "source": [
        "del pricefeats_df\n",
        "del grid_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNjbwkypP760"
      },
      "source": [
        "*   CREATE ENCODING FEATURES\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PSvyjOKQJKS"
      },
      "outputs": [],
      "source": [
        "encoding_df = pd.read_pickle(grid_base_path)\n",
        "encoding_df.loc[encoding_df['week']>(end_train_week_x),target] = np.nan\n",
        "                       \n",
        "base_cols = list(encoding_df)\n",
        "\n",
        "icols = [['cat_id'],\n",
        "    ['dept_id'],\n",
        "    ['item_id']]\n",
        "\n",
        "for col in icols:\n",
        "    col_name = '_' + '_'.join(col) + '_'\n",
        "    encoding_df['enc' + col_name + 'mean'] = encoding_df.groupby(col)[target].transform('mean').astype(\n",
        "        np.float16)\n",
        "    encoding_df['enc' + col_name + 'std'] = encoding_df.groupby(col)[target].transform('std').astype(\n",
        "        np.float16)\n",
        "\n",
        "keep_cols = [col for col in list(encoding_df) if col not in base_cols]\n",
        "encoding_df = encoding_df[['id', 'wm_yr_wk'] + keep_cols]\n",
        "\n",
        "encoding_df.to_pickle(encoding_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVbeN1f1hYc0"
      },
      "outputs": [],
      "source": [
        "del encoding_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btuBOAiQPn5e"
      },
      "source": [
        "**CREATE LAG FEATURES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cMCE--mP624"
      },
      "outputs": [],
      "source": [
        "for prediction_horizon in [3,6,9,12]:\n",
        "    num_lag_week_list = []\n",
        "    num_lag_week = 12\n",
        "    for col in range(prediction_horizon, prediction_horizon + num_lag_week):\n",
        "        num_lag_week_list.append(col)\n",
        "    num_rolling_week_list = [3, 6, 12, 24, 48]\n",
        "\n",
        "    lagfeats_df = pd.read_pickle(grid_base_path)\n",
        "    lagfeats_df = lagfeats_df[['id', 'week',target]]\n",
        "    lagfeats_df.loc[lagfeats_df['week']>end_train_week_x,target] = np.nan\n",
        "\n",
        "    lagfeats_df = lagfeats_df.assign(**{\n",
        "        '{}_lag_{}'.format(col, l): lagfeats_df.groupby(['id'])[col].transform(lambda x: x.shift(l))\n",
        "        for l in num_lag_week_list\n",
        "        for col in [target]\n",
        "    })\n",
        "\n",
        "    for col in list(lagfeats_df):\n",
        "        if 'lag' in col:\n",
        "            lagfeats_df[col] = lagfeats_df[col].astype(np.float16)\n",
        "\n",
        "    for num_rolling_week in num_rolling_week_list:\n",
        "        lagfeats_df['rolling_mean_' + str(num_rolling_week)] = lagfeats_df.groupby(['id'])[target].transform(\n",
        "            lambda x: x.shift(prediction_horizon).rolling(num_rolling_week).mean()).astype(np.float16)\n",
        "        lagfeats_df['rolling_std_' + str(num_rolling_week)] = lagfeats_df.groupby(['id'])[target].transform(\n",
        "            lambda x: x.shift(prediction_horizon).rolling(num_rolling_week).std()).astype(np.float16)\n",
        "\n",
        "    lagfeats_df.to_pickle(lagfeats_path+str(prediction_horizon))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYgjEtkIYvCk"
      },
      "source": [
        "**CREATE FULL DATASET BY STORE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiX04tdpYxtk"
      },
      "outputs": [],
      "source": [
        "def load_df(store_id,end_train_week_x,prediction_horizon):\n",
        "    \n",
        "    grid_base = pd.read_pickle(grid_base_path)\n",
        "    \n",
        "\n",
        "    pricefeats = pd.read_pickle(pricefeats_path)\n",
        "    calfeats = pd.read_pickle(calfeats_path)\n",
        "    encodingfeats = pd.read_pickle(encoding_path)\n",
        "    \n",
        "    full_df = pd.concat([grid_base,\n",
        "                         pricefeats.iloc[:, 2:],\n",
        "                         calfeats.iloc[:, 2:],\n",
        "                         encodingfeats.iloc[:, 2:]],\n",
        "                        axis=1)\n",
        "\n",
        "    del grid_base,pricefeats,calfeats,encodingfeats\n",
        "\n",
        "    if store_id != 'all':\n",
        "            full_df = full_df[full_df['store_id'] == store_id]\n",
        "\n",
        "    full_df = full_df[full_df['week']<=(end_train_week_x+prediction_horizon)]\n",
        "\n",
        "    lagfeats = pd.read_pickle(lagfeats_path+str(prediction_horizon))\n",
        "    lagfeats = lagfeats.iloc[:, 3:]\n",
        "    lagfeats= lagfeats[lagfeats.index.isin(full_df.index)]\n",
        "\n",
        "    full_df = pd.concat([full_df, lagfeats], axis=1)\n",
        "    del lagfeats\n",
        "\n",
        "    enable_features = [col for col in list(full_df) if col not in remove_features]\n",
        "    full_df = full_df[['id', 'week', target] + enable_features]\n",
        "\n",
        "    #full_df = full_df[full_df['week'] >= start_train_week_x].reset_index(drop=True)\n",
        "\n",
        "    return full_df, enable_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx0gmT8yqyWz"
      },
      "source": [
        "**RUN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEXQXABnq5q2",
        "outputId": "552b8472-a054-4db5-d94b-e3399731d6f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1386     7\n",
            "1387    12\n",
            "1388     6\n",
            "1389     6\n",
            "1390     7\n",
            "Name: demand, dtype: int64\n",
            "[100]\tvalid_0's rmse: 10.344\n",
            "[200]\tvalid_0's rmse: 8.90512\n",
            "[300]\tvalid_0's rmse: 8.76908\n",
            "[400]\tvalid_0's rmse: 8.71102\n",
            "[500]\tvalid_0's rmse: 8.64387\n",
            "[600]\tvalid_0's rmse: 8.58386\n",
            "[700]\tvalid_0's rmse: 8.49066\n",
            "[800]\tvalid_0's rmse: 8.44113\n",
            "[900]\tvalid_0's rmse: 8.37158\n",
            "[1000]\tvalid_0's rmse: 8.32323\n",
            "[1100]\tvalid_0's rmse: 8.26932\n",
            "[1200]\tvalid_0's rmse: 8.19554\n",
            "[1300]\tvalid_0's rmse: 8.1775\n",
            "[1400]\tvalid_0's rmse: 8.13107\n",
            "Our val rmse score is 8.131070016533705\n",
            "[4.40507666 4.45816883 4.14997417 ... 1.76756604 1.55807197 1.67722264]\n",
            "                                      id  week    demand\n",
            "1650         FOODS_1_001_TX_2_evaluation   265  4.405077\n",
            "1651         FOODS_1_001_TX_2_evaluation   266  4.458169\n",
            "1652         FOODS_1_001_TX_2_evaluation   267  4.149974\n",
            "4420         FOODS_1_002_TX_2_evaluation   265  1.429923\n",
            "4421         FOODS_1_002_TX_2_evaluation   266  1.293326\n",
            "...                                  ...   ...       ...\n",
            "8441841  HOUSEHOLD_2_515_TX_2_evaluation   266  1.135095\n",
            "8441842  HOUSEHOLD_2_515_TX_2_evaluation   267  1.040571\n",
            "8444610  HOUSEHOLD_2_516_TX_2_evaluation   265  1.767566\n",
            "8444611  HOUSEHOLD_2_516_TX_2_evaluation   266  1.558072\n",
            "8444612  HOUSEHOLD_2_516_TX_2_evaluation   267  1.677223\n",
            "\n",
            "[9147 rows x 3 columns]\n",
            "1386     7\n",
            "1387    12\n",
            "1388     6\n",
            "1389     6\n",
            "1390     7\n",
            "Name: demand, dtype: int64\n",
            "[100]\tvalid_0's rmse: 10.7859\n",
            "[200]\tvalid_0's rmse: 9.82886\n",
            "[300]\tvalid_0's rmse: 9.58618\n",
            "[400]\tvalid_0's rmse: 9.31016\n",
            "[500]\tvalid_0's rmse: 9.09521\n",
            "[600]\tvalid_0's rmse: 8.93105\n",
            "[700]\tvalid_0's rmse: 8.79325\n",
            "[800]\tvalid_0's rmse: 8.67351\n",
            "[900]\tvalid_0's rmse: 8.57528\n",
            "[1000]\tvalid_0's rmse: 8.48759\n",
            "[1100]\tvalid_0's rmse: 8.41267\n",
            "[1200]\tvalid_0's rmse: 8.35001\n",
            "[1300]\tvalid_0's rmse: 8.30399\n",
            "[1400]\tvalid_0's rmse: 8.24602\n",
            "Our val rmse score is 8.246018986026327\n",
            "[3.57214717 4.06085273 3.55959979 ... 1.73248364 1.69855946 1.83426614]\n",
            "                                      id  week    demand\n",
            "1650         FOODS_1_001_TX_2_evaluation   265  3.572147\n",
            "1651         FOODS_1_001_TX_2_evaluation   266  4.060853\n",
            "1652         FOODS_1_001_TX_2_evaluation   267  3.559600\n",
            "1653         FOODS_1_001_TX_2_evaluation   268  3.813869\n",
            "1654         FOODS_1_001_TX_2_evaluation   269  4.034476\n",
            "...                                  ...   ...       ...\n",
            "8444611  HOUSEHOLD_2_516_TX_2_evaluation   266  1.966154\n",
            "8444612  HOUSEHOLD_2_516_TX_2_evaluation   267  1.820355\n",
            "8444613  HOUSEHOLD_2_516_TX_2_evaluation   268  1.732484\n",
            "8444614  HOUSEHOLD_2_516_TX_2_evaluation   269  1.698559\n",
            "8444615  HOUSEHOLD_2_516_TX_2_evaluation   270  1.834266\n",
            "\n",
            "[18294 rows x 3 columns]\n",
            "1386     7\n",
            "1387    12\n",
            "1388     6\n",
            "1389     6\n",
            "1390     7\n",
            "Name: demand, dtype: int64\n",
            "[100]\tvalid_0's rmse: 10.7562\n",
            "[200]\tvalid_0's rmse: 10.2821\n",
            "[300]\tvalid_0's rmse: 10.0856\n",
            "[400]\tvalid_0's rmse: 9.9298\n",
            "[500]\tvalid_0's rmse: 9.74536\n",
            "[600]\tvalid_0's rmse: 9.61286\n",
            "[700]\tvalid_0's rmse: 9.50256\n",
            "[800]\tvalid_0's rmse: 9.39364\n",
            "[900]\tvalid_0's rmse: 9.29834\n",
            "[1000]\tvalid_0's rmse: 9.21853\n",
            "[1100]\tvalid_0's rmse: 9.1429\n",
            "[1200]\tvalid_0's rmse: 9.04832\n",
            "[1300]\tvalid_0's rmse: 8.98888\n",
            "[1400]\tvalid_0's rmse: 8.92533\n",
            "Our val rmse score is 8.925331147419207\n",
            "[3.6868638  3.81928765 3.36831561 ... 1.60446335 1.60477752 1.71744084]\n",
            "                                      id  week    demand\n",
            "1650         FOODS_1_001_TX_2_evaluation   265  3.686864\n",
            "1651         FOODS_1_001_TX_2_evaluation   266  3.819288\n",
            "1652         FOODS_1_001_TX_2_evaluation   267  3.368316\n",
            "1653         FOODS_1_001_TX_2_evaluation   268  3.120769\n",
            "1654         FOODS_1_001_TX_2_evaluation   269  3.453516\n",
            "...                                  ...   ...       ...\n",
            "8444614  HOUSEHOLD_2_516_TX_2_evaluation   269  1.886712\n",
            "8444615  HOUSEHOLD_2_516_TX_2_evaluation   270  1.751964\n",
            "8444616  HOUSEHOLD_2_516_TX_2_evaluation   271  1.604463\n",
            "8444617  HOUSEHOLD_2_516_TX_2_evaluation   272  1.604778\n",
            "8444618  HOUSEHOLD_2_516_TX_2_evaluation   273  1.717441\n",
            "\n",
            "[27441 rows x 3 columns]\n",
            "1386     7\n",
            "1387    12\n",
            "1388     6\n",
            "1389     6\n",
            "1390     7\n",
            "Name: demand, dtype: int64\n",
            "[100]\tvalid_0's rmse: 11.8697\n",
            "[200]\tvalid_0's rmse: 11.1164\n",
            "[300]\tvalid_0's rmse: 10.8302\n",
            "[400]\tvalid_0's rmse: 10.6575\n",
            "[500]\tvalid_0's rmse: 10.4581\n",
            "[600]\tvalid_0's rmse: 10.3427\n",
            "[700]\tvalid_0's rmse: 10.208\n",
            "[800]\tvalid_0's rmse: 10.1009\n",
            "[900]\tvalid_0's rmse: 9.95295\n",
            "[1000]\tvalid_0's rmse: 9.85536\n",
            "[1100]\tvalid_0's rmse: 9.72735\n",
            "[1200]\tvalid_0's rmse: 9.63599\n",
            "[1300]\tvalid_0's rmse: 9.57781\n",
            "[1400]\tvalid_0's rmse: 9.50932\n",
            "Our val rmse score is 9.509323516073502\n",
            "[3.47757222 3.75501896 3.70946158 ... 1.55816591 1.44184508 1.60491104]\n",
            "                                      id  week    demand\n",
            "1650         FOODS_1_001_TX_2_evaluation   265  3.477572\n",
            "1651         FOODS_1_001_TX_2_evaluation   266  3.755019\n",
            "1652         FOODS_1_001_TX_2_evaluation   267  3.709462\n",
            "1653         FOODS_1_001_TX_2_evaluation   268  3.238592\n",
            "1654         FOODS_1_001_TX_2_evaluation   269  3.569940\n",
            "...                                  ...   ...       ...\n",
            "8444617  HOUSEHOLD_2_516_TX_2_evaluation   272  1.578649\n",
            "8444618  HOUSEHOLD_2_516_TX_2_evaluation   273  1.586625\n",
            "8444619  HOUSEHOLD_2_516_TX_2_evaluation   274  1.558166\n",
            "8444620  HOUSEHOLD_2_516_TX_2_evaluation   275  1.441845\n",
            "8444621  HOUSEHOLD_2_516_TX_2_evaluation   276  1.604911\n",
            "\n",
            "[36588 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "lgb_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'tweedie',\n",
        "    'tweedie_variance_power': 1.1,\n",
        "    'metric': 'rmse',\n",
        "    'subsample': 0.5,\n",
        "    'subsample_freq': 1,\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 2 ** 11 - 1,\n",
        "    'min_data_in_leaf': 2 ** 12 - 1,\n",
        "    'feature_fraction': 0.5,\n",
        "    'max_bin': 100,\n",
        "    'n_estimators': 1400,\n",
        "    'boost_from_average': False,\n",
        "}\n",
        "\n",
        "store_id_set_list = list(train_df['store_id'].unique())\n",
        "\n",
        "for store_index, store_id in enumerate(store_id_set_list):\n",
        "    for prediction_horizon in [3,6,9,12]:\n",
        "      save_name = '/content/drive/MyDrive/Capstone/Models/Weekly_lightgbm/weekly_lightgbm/' + str(store_id)+'-'+str(prediction_horizon)+'-'+'.csv'\n",
        "    \n",
        "      grid_df,enable_features = load_df(store_id,end_train_week_x,prediction_horizon)\n",
        "\n",
        "      x_train = grid_df[(grid_df['week'] >= start_train_week_x) & (grid_df['week'] <= end_train_week_x)]\n",
        "      y_train = x_train[target]\n",
        "      print(y_train.head())\n",
        "      x_val = grid_df[(grid_df['week'] > (end_train_week_x - prediction_horizon)) & (grid_df['week'] <= end_train_week_x)]\n",
        "      y_val = x_val[target]\n",
        "      test = grid_df[grid_df['week'] > end_train_week_x]\n",
        "      \n",
        "      train_data = lgb.Dataset(x_train[enable_features],\n",
        "                                      label=y_train)\n",
        "      \n",
        "      val_data = lgb.Dataset(x_val[enable_features],\n",
        "                                      label=y_val)\n",
        "      \n",
        "      del grid_df, x_train, y_train\n",
        "      gc.collect()\n",
        "      \n",
        "      estimator = lgb.train(lgb_params,train_data,valid_sets = [val_data], verbose_eval = 100)\n",
        "      \n",
        "      val_pred = estimator.predict(x_val[enable_features])\n",
        "      val_score = np.sqrt(mean_squared_error(val_pred, y_val))\n",
        "      print(f'Our val rmse score is {val_score}')\n",
        "\n",
        "      y_pred = estimator.predict(test[enable_features])\n",
        "      test[target] = y_pred\n",
        "      print(y_pred)\n",
        "\n",
        "      predictions = test[['id', 'week', target]]\n",
        "      print(predictions)\n",
        "      predictions = pd.pivot(predictions, index = 'id', columns = 'week', values = target).reset_index()\n",
        "\n",
        "      predictions.to_csv(save_name,index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}